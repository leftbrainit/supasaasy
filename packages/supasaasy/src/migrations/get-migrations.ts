/**
 * Migration Generation
 *
 * Generates SQL migrations for SupaSaaSy based on the configuration.
 * Combines core schema with connector-specific migrations.
 */

import type { SupaSaaSyConfig } from '../types/index.ts';
import { getConnector } from '../connectors/index.ts';

// Import connectors to ensure they register themselves
import '../connectors/stripe/index.ts';
import '../connectors/intercom/index.ts';
import '../connectors/notion/index.ts';

// =============================================================================
// Core Schema SQL
// =============================================================================

const CORE_SCHEMA_SQL =
  `-- ============================================================================
-- SupaSaaSy Core Schema
-- Generated by @supasaasy/core
-- ============================================================================

-- Create the supasaasy schema
CREATE SCHEMA IF NOT EXISTS supasaasy;

-- Grant usage to authenticated and service_role
GRANT USAGE ON SCHEMA supasaasy TO authenticated;
GRANT USAGE ON SCHEMA supasaasy TO service_role;

-- Set default privileges for future tables
ALTER DEFAULT PRIVILEGES IN SCHEMA supasaasy
GRANT SELECT ON TABLES TO authenticated;

ALTER DEFAULT PRIVILEGES IN SCHEMA supasaasy
GRANT ALL ON TABLES TO service_role;

-- Comment on schema
COMMENT ON SCHEMA supasaasy IS 'SupaSaaSy data sync framework schema';

-- =============================================================================
-- Canonical Entities Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.entities (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  external_id TEXT NOT NULL,
  app_key TEXT NOT NULL,
  collection_key TEXT NOT NULL,
  api_version TEXT,
  raw_payload JSONB NOT NULL DEFAULT '{}',
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  archived_at TIMESTAMPTZ,
  deleted_at TIMESTAMPTZ
);

COMMENT ON TABLE supasaasy.entities IS 'Canonical storage for all synced SaaS entities';

-- Unique constraint for idempotent upserts
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'entities_app_collection_external_unique'
  ) THEN
    ALTER TABLE supasaasy.entities
    ADD CONSTRAINT entities_app_collection_external_unique
    UNIQUE (app_key, collection_key, external_id);
  END IF;
END $$;

-- Indexes
CREATE INDEX IF NOT EXISTS idx_entities_app_key ON supasaasy.entities (app_key);
CREATE INDEX IF NOT EXISTS idx_entities_collection_key ON supasaasy.entities (collection_key);
CREATE INDEX IF NOT EXISTS idx_entities_external_id ON supasaasy.entities (external_id);
CREATE INDEX IF NOT EXISTS idx_entities_app_collection ON supasaasy.entities (app_key, collection_key);
CREATE INDEX IF NOT EXISTS idx_entities_updated_at ON supasaasy.entities (updated_at DESC);

-- Timestamp auto-update trigger
CREATE OR REPLACE FUNCTION supasaasy.update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = now();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS entities_update_updated_at ON supasaasy.entities;
CREATE TRIGGER entities_update_updated_at
  BEFORE UPDATE ON supasaasy.entities
  FOR EACH ROW
  EXECUTE FUNCTION supasaasy.update_updated_at_column();

-- Grant permissions
GRANT SELECT ON supasaasy.entities TO authenticated;
GRANT ALL ON supasaasy.entities TO service_role;

-- =============================================================================
-- Sync State Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.sync_state (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  app_key TEXT NOT NULL,
  collection_key TEXT NOT NULL,
  last_synced_at TIMESTAMPTZ NOT NULL,
  last_sync_metadata JSONB NOT NULL DEFAULT '{}',
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

COMMENT ON TABLE supasaasy.sync_state IS 'Tracks sync state for each app/collection combination';

-- Unique constraint
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'sync_state_app_collection_unique'
  ) THEN
    ALTER TABLE supasaasy.sync_state
    ADD CONSTRAINT sync_state_app_collection_unique
    UNIQUE (app_key, collection_key);
  END IF;
END $$;

-- Indexes
CREATE INDEX IF NOT EXISTS idx_sync_state_app_key ON supasaasy.sync_state (app_key);

-- Trigger for updated_at
DROP TRIGGER IF EXISTS sync_state_update_updated_at ON supasaasy.sync_state;
CREATE TRIGGER sync_state_update_updated_at
  BEFORE UPDATE ON supasaasy.sync_state
  FOR EACH ROW
  EXECUTE FUNCTION supasaasy.update_updated_at_column();

-- Grant permissions
GRANT SELECT ON supasaasy.sync_state TO authenticated;
GRANT ALL ON supasaasy.sync_state TO service_role;

-- =============================================================================
-- Webhook Logs Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.webhook_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  app_key TEXT,
  request_method TEXT NOT NULL,
  request_path TEXT NOT NULL,
  request_headers JSONB NOT NULL DEFAULT '{}',
  request_body JSONB,
  response_status INTEGER NOT NULL,
  response_body JSONB,
  error_message TEXT,
  processing_duration_ms INTEGER,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

COMMENT ON TABLE supasaasy.webhook_logs IS 'Logs webhook requests for debugging and auditing';
COMMENT ON COLUMN supasaasy.webhook_logs.app_key IS 'App key the webhook was received for (nullable for invalid requests)';
COMMENT ON COLUMN supasaasy.webhook_logs.request_method IS 'HTTP method of the webhook request';
COMMENT ON COLUMN supasaasy.webhook_logs.request_path IS 'URL path of the webhook request';
COMMENT ON COLUMN supasaasy.webhook_logs.request_headers IS 'Request headers (sensitive values redacted)';
COMMENT ON COLUMN supasaasy.webhook_logs.request_body IS 'Request body payload';
COMMENT ON COLUMN supasaasy.webhook_logs.response_status IS 'HTTP response status code';
COMMENT ON COLUMN supasaasy.webhook_logs.response_body IS 'Response body payload';
COMMENT ON COLUMN supasaasy.webhook_logs.error_message IS 'Error message if processing failed';
COMMENT ON COLUMN supasaasy.webhook_logs.processing_duration_ms IS 'Processing duration in milliseconds';
COMMENT ON COLUMN supasaasy.webhook_logs.created_at IS 'Timestamp when the log entry was created';

-- Indexes for efficient querying
CREATE INDEX IF NOT EXISTS idx_webhook_logs_app_key ON supasaasy.webhook_logs (app_key);
CREATE INDEX IF NOT EXISTS idx_webhook_logs_response_status ON supasaasy.webhook_logs (response_status);
CREATE INDEX IF NOT EXISTS idx_webhook_logs_created_at ON supasaasy.webhook_logs (created_at DESC);
CREATE INDEX IF NOT EXISTS idx_webhook_logs_app_key_created_at ON supasaasy.webhook_logs (app_key, created_at DESC);

-- Grant permissions
GRANT SELECT ON supasaasy.webhook_logs TO authenticated;
GRANT ALL ON supasaasy.webhook_logs TO service_role;

-- =============================================================================
-- Sync Jobs Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.sync_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  app_key TEXT NOT NULL,
  mode TEXT NOT NULL,
  resource_types TEXT[],
  status TEXT NOT NULL DEFAULT 'pending',
  total_tasks INTEGER NOT NULL DEFAULT 0,
  completed_tasks INTEGER NOT NULL DEFAULT 0,
  failed_tasks INTEGER NOT NULL DEFAULT 0,
  processed_entities INTEGER NOT NULL DEFAULT 0,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  error_message TEXT,
  -- Worker continuation support
  needs_worker BOOLEAN NOT NULL DEFAULT true,
  worker_spawned_at TIMESTAMPTZ
);

COMMENT ON TABLE supasaasy.sync_jobs IS 'Tracks batch sync job metadata and progress';
COMMENT ON COLUMN supasaasy.sync_jobs.app_key IS 'App key the job is running for';
COMMENT ON COLUMN supasaasy.sync_jobs.mode IS 'Sync mode: full or incremental';
COMMENT ON COLUMN supasaasy.sync_jobs.resource_types IS 'Array of resource types being synced';
COMMENT ON COLUMN supasaasy.sync_jobs.status IS 'Job status: pending, processing, completed, failed, cancelled';
COMMENT ON COLUMN supasaasy.sync_jobs.total_tasks IS 'Total number of resource sync tasks for this job';
COMMENT ON COLUMN supasaasy.sync_jobs.completed_tasks IS 'Number of tasks successfully completed';
COMMENT ON COLUMN supasaasy.sync_jobs.failed_tasks IS 'Number of tasks that failed';
COMMENT ON COLUMN supasaasy.sync_jobs.processed_entities IS 'Total number of entities processed across all tasks';
COMMENT ON COLUMN supasaasy.sync_jobs.needs_worker IS 'Flag to trigger worker spawning via pg_net';
COMMENT ON COLUMN supasaasy.sync_jobs.worker_spawned_at IS 'Timestamp of last worker spawn attempt';

-- Status check constraint
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'sync_jobs_status_check'
  ) THEN
    ALTER TABLE supasaasy.sync_jobs
    ADD CONSTRAINT sync_jobs_status_check
    CHECK (status IN ('pending', 'processing', 'completed', 'failed', 'cancelled'));
  END IF;
END $$;

-- Indexes
CREATE INDEX IF NOT EXISTS idx_sync_jobs_id ON supasaasy.sync_jobs (id);
CREATE INDEX IF NOT EXISTS idx_sync_jobs_app_key ON supasaasy.sync_jobs (app_key);
CREATE INDEX IF NOT EXISTS idx_sync_jobs_status ON supasaasy.sync_jobs (status);
CREATE INDEX IF NOT EXISTS idx_sync_jobs_created_at ON supasaasy.sync_jobs (created_at DESC);

-- Grant permissions
GRANT SELECT ON supasaasy.sync_jobs TO authenticated;
GRANT ALL ON supasaasy.sync_jobs TO service_role;

-- =============================================================================
-- Sync Job Tasks Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.sync_job_tasks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  job_id UUID NOT NULL,
  resource_type TEXT NOT NULL,
  status TEXT NOT NULL DEFAULT 'pending',
  entity_count INTEGER,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  started_at TIMESTAMPTZ,
  completed_at TIMESTAMPTZ,
  error_message TEXT,
  -- Pagination cursor for resumable sync
  cursor TEXT,
  -- Heartbeat for detecting stuck tasks
  last_heartbeat TIMESTAMPTZ
);

COMMENT ON TABLE supasaasy.sync_job_tasks IS 'Individual resource sync tasks for batch sync jobs';
COMMENT ON COLUMN supasaasy.sync_job_tasks.job_id IS 'Foreign key to sync_jobs';
COMMENT ON COLUMN supasaasy.sync_job_tasks.resource_type IS 'Resource type this task syncs (e.g., customer, subscription)';
COMMENT ON COLUMN supasaasy.sync_job_tasks.status IS 'Task status: pending, processing, completed, failed';
COMMENT ON COLUMN supasaasy.sync_job_tasks.entity_count IS 'Number of entities processed by this task';
COMMENT ON COLUMN supasaasy.sync_job_tasks.cursor IS 'Pagination cursor for resuming interrupted syncs';
COMMENT ON COLUMN supasaasy.sync_job_tasks.last_heartbeat IS 'Last heartbeat timestamp from worker processing this task';

-- Foreign key constraint with cascade delete
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'sync_job_tasks_job_id_fkey'
  ) THEN
    ALTER TABLE supasaasy.sync_job_tasks
    ADD CONSTRAINT sync_job_tasks_job_id_fkey
    FOREIGN KEY (job_id) REFERENCES supasaasy.sync_jobs(id) ON DELETE CASCADE;
  END IF;
END $$;

-- Status check constraint
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'sync_job_tasks_status_check'
  ) THEN
    ALTER TABLE supasaasy.sync_job_tasks
    ADD CONSTRAINT sync_job_tasks_status_check
    CHECK (status IN ('pending', 'processing', 'completed', 'failed'));
  END IF;
END $$;

-- Unique constraint on job_id and resource_type (one task per resource per job)
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'sync_job_tasks_job_resource_unique'
  ) THEN
    ALTER TABLE supasaasy.sync_job_tasks
    ADD CONSTRAINT sync_job_tasks_job_resource_unique
    UNIQUE (job_id, resource_type);
  END IF;
END $$;

-- Indexes
CREATE INDEX IF NOT EXISTS idx_sync_job_tasks_job_id ON supasaasy.sync_job_tasks (job_id);
CREATE INDEX IF NOT EXISTS idx_sync_job_tasks_status ON supasaasy.sync_job_tasks (status);

-- Grant permissions
GRANT SELECT ON supasaasy.sync_job_tasks TO authenticated;
GRANT ALL ON supasaasy.sync_job_tasks TO service_role;

-- =============================================================================
-- Worker Auto-Spawn Trigger (requires pg_net extension)
-- =============================================================================
-- This trigger automatically spawns a worker Edge Function when:
-- 1. A new job is created with needs_worker = true
-- 2. An existing job has needs_worker set to true (for worker chaining)
--
-- Prerequisites:
-- 1. Enable pg_net extension: CREATE EXTENSION IF NOT EXISTS pg_net;
-- 2. Set SUPABASE_URL in vault or as a database setting
-- 3. Set ADMIN_API_KEY in vault or as a database setting
--
-- The trigger uses pg_net.http_post to asynchronously call the worker function.
-- =============================================================================

-- Enable pg_net extension if not already enabled
CREATE EXTENSION IF NOT EXISTS pg_net WITH SCHEMA extensions;

-- Function to spawn a worker via pg_net
CREATE OR REPLACE FUNCTION supasaasy.spawn_worker()
RETURNS TRIGGER AS $$
DECLARE
  supabase_url TEXT;
  admin_api_key TEXT;
  worker_url TEXT;
  request_body TEXT;
BEGIN
  -- Only spawn if needs_worker is true and job is not completed/failed/cancelled
  IF NEW.needs_worker = true AND NEW.status NOT IN ('completed', 'failed', 'cancelled') THEN
    -- Get configuration from environment or vault
    -- Try to get from current_setting first (can be set via SET command or config)
    BEGIN
      supabase_url := current_setting('app.supabase_url', true);
    EXCEPTION WHEN OTHERS THEN
      supabase_url := NULL;
    END;
    
    BEGIN
      admin_api_key := current_setting('app.admin_api_key', true);
    EXCEPTION WHEN OTHERS THEN
      admin_api_key := NULL;
    END;
    
    -- If not found, check for common environment patterns
    IF supabase_url IS NULL THEN
      -- For local development, default to localhost
      supabase_url := COALESCE(
        current_setting('app.settings.supabase_url', true),
        'http://127.0.0.1:54321'
      );
    END IF;
    
    IF admin_api_key IS NULL THEN
      admin_api_key := COALESCE(
        current_setting('app.settings.admin_api_key', true),
        ''
      );
    END IF;
    
    -- Build worker URL
    worker_url := supabase_url || '/functions/v1/worker';
    
    -- Build request body
    request_body := json_build_object('job_id', NEW.id)::TEXT;
    
    -- Only attempt to spawn if we have credentials
    IF admin_api_key IS NOT NULL AND admin_api_key != '' THEN
      -- Make async HTTP request to worker function
      PERFORM extensions.http_post(
        url := worker_url,
        body := request_body::JSONB,
        headers := json_build_object(
          'Content-Type', 'application/json',
          'Authorization', 'Bearer ' || admin_api_key
        )::JSONB
      );
      
      -- Update spawn timestamp and reset flag
      NEW.worker_spawned_at := now();
      NEW.needs_worker := false;
      
      RAISE LOG 'supasaasy: Spawned worker for job %', NEW.id;
    ELSE
      RAISE WARNING 'supasaasy: Cannot spawn worker - admin_api_key not configured';
    END IF;
  END IF;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Create trigger for auto-spawning workers (idempotent)
DO $$
BEGIN
  -- Drop existing trigger if it exists (wrapped to suppress NOTICE)
  IF EXISTS (
    SELECT 1 FROM pg_trigger
    WHERE tgname = 'spawn_worker_on_job_change'
  ) THEN
    DROP TRIGGER spawn_worker_on_job_change ON supasaasy.sync_jobs;
  END IF;
  
  -- Create the trigger
  CREATE TRIGGER spawn_worker_on_job_change
    BEFORE INSERT OR UPDATE OF needs_worker ON supasaasy.sync_jobs
    FOR EACH ROW
    EXECUTE FUNCTION supasaasy.spawn_worker();
END $$;

COMMENT ON FUNCTION supasaasy.spawn_worker() IS 'Automatically spawns worker Edge Function when needs_worker flag is set';
`;

// =============================================================================
// Migration Generation
// =============================================================================

/**
 * Options for getMigrations
 */
export interface GetMigrationsOptions {
  /**
   * Whether to include a header comment with version and timestamp.
   * Default: true
   */
  includeHeader?: boolean;

  /**
   * Custom version string to include in the header.
   * Default: package version
   */
  version?: string;
}

/**
 * Generate SQL migrations for SupaSaaSy based on the configuration.
 *
 * This function generates a complete SQL migration file that includes:
 * 1. The core SupaSaaSy schema (entities table, sync_state table, webhook_logs table)
 * 2. Connector-specific migrations for all connectors used in the configuration
 *
 * The generated SQL uses idempotent statements (CREATE IF NOT EXISTS, CREATE OR REPLACE)
 * so it can be safely re-run.
 *
 * @param config The SupaSaaSy configuration
 * @param options Generation options
 * @returns Complete SQL migration string
 *
 * @example
 * ```typescript
 * import { getMigrations } from 'supasaasy';
 * import config from '../supasaasy.config.ts';
 *
 * const sql = getMigrations(config);
 * await Deno.writeTextFile(
 *   'supabase/migrations/00000000000001_supasaasy.sql',
 *   sql
 * );
 * console.log('Migration file generated');
 * ```
 */
export async function getMigrations(
  config: SupaSaaSyConfig,
  options: GetMigrationsOptions = {},
): Promise<string> {
  const { includeHeader = true, version = '1.0.0' } = options;

  const parts: string[] = [];

  // Add header if requested
  if (includeHeader) {
    parts.push(`-- ============================================================================
-- SupaSaaSy Migration
-- Version: ${version}
-- Generated: ${new Date().toISOString()}
-- ============================================================================
-- This migration file was generated by @supasaasy/core getMigrations().
-- It includes the core schema and connector-specific migrations.
-- All statements are idempotent and can be safely re-run.
-- ============================================================================
`);
  }

  // Add core schema
  parts.push(CORE_SCHEMA_SQL);

  // Collect unique connectors from config
  const connectorNames = new Set(config.apps.map((app) => app.connector));

  // Add connector-specific migrations
  for (const connectorName of connectorNames) {
    const connector = await getConnector(connectorName);
    if (!connector) {
      console.warn(`Connector not found: ${connectorName}`);
      continue;
    }

    const migrationFiles = connector.metadata.migrations || [];
    if (migrationFiles.length === 0) {
      continue;
    }

    parts.push(`
-- ============================================================================
-- ${connector.metadata.displayName} Connector Migrations
-- Version: ${connector.metadata.version}
-- ============================================================================
`);

    // Load and append each migration file
    // In the library context, migrations are bundled as text
    for (const _filename of migrationFiles) {
      // The actual SQL content would need to be embedded or loaded
      // For now, we'll reference that connectors should export their migration SQL
      const migrationSql = await loadConnectorMigration(connectorName, _filename);
      if (migrationSql) {
        parts.push(migrationSql);
      }
    }
  }

  return parts.join('\n');
}

/**
 * Load a connector's migration SQL file.
 * This is a helper that reads migration files from the package.
 */
async function loadConnectorMigration(
  connectorName: string,
  filename: string,
): Promise<string | null> {
  // In the library context, we need to load migration files
  // This can be done via dynamic import or by embedding the SQL

  // Try to load from the package's migrations directory
  // The path is relative to the module's location
  const migrationPaths = [
    // Relative to this file
    `../connectors/${connectorName}/migrations/${filename}`,
    // Absolute path for development
    `./src/connectors/${connectorName}/migrations/${filename}`,
  ];

  for (const path of migrationPaths) {
    try {
      // Try to construct the full URL for the migration file
      const url = new URL(path, import.meta.url);
      const response = await fetch(url);
      if (response.ok) {
        return await response.text();
      }
    } catch {
      // Try reading as file path
      try {
        const modulePath = import.meta.url.replace('file://', '').replace('/get-migrations.ts', '');
        const filePath = `${modulePath}/../connectors/${connectorName}/migrations/${filename}`;
        return await Deno.readTextFile(filePath);
      } catch {
        // Continue to next path
      }
    }
  }

  console.warn(`Could not load migration file: ${connectorName}/${filename}`);
  return null;
}

/**
 * Get the core schema SQL without any connector-specific migrations.
 * Useful for minimal setups or testing.
 */
export function getCoreSchema(): string {
  return CORE_SCHEMA_SQL;
}
