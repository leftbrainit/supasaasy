/**
 * Migration Generation
 *
 * Generates SQL migrations for SupaSaaSy based on the configuration.
 * Combines core schema with connector-specific migrations.
 */

import type { SupaSaaSyConfig } from '../types/index.ts';
import { getConnector } from '../connectors/index.ts';

// Import connectors to ensure they register themselves
import '../connectors/stripe/index.ts';
import '../connectors/intercom/index.ts';
import '../connectors/notion/index.ts';

// =============================================================================
// Core Schema SQL
// =============================================================================

const CORE_SCHEMA_SQL =
  `-- ============================================================================
-- SupaSaaSy Core Schema
-- Generated by @supasaasy/core
-- ============================================================================

-- Create the supasaasy schema
CREATE SCHEMA IF NOT EXISTS supasaasy;

-- Grant usage to authenticated and service_role
GRANT USAGE ON SCHEMA supasaasy TO authenticated;
GRANT USAGE ON SCHEMA supasaasy TO service_role;

-- Set default privileges for future tables
ALTER DEFAULT PRIVILEGES IN SCHEMA supasaasy
GRANT SELECT ON TABLES TO authenticated;

ALTER DEFAULT PRIVILEGES IN SCHEMA supasaasy
GRANT ALL ON TABLES TO service_role;

-- Comment on schema
COMMENT ON SCHEMA supasaasy IS 'SupaSaaSy data sync framework schema';

-- =============================================================================
-- Canonical Entities Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.entities (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  external_id TEXT NOT NULL,
  app_key TEXT NOT NULL,
  collection_key TEXT NOT NULL,
  api_version TEXT,
  raw_payload JSONB NOT NULL DEFAULT '{}',
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  archived_at TIMESTAMPTZ,
  deleted_at TIMESTAMPTZ
);

COMMENT ON TABLE supasaasy.entities IS 'Canonical storage for all synced SaaS entities';

-- Unique constraint for idempotent upserts
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'entities_app_collection_external_unique'
  ) THEN
    ALTER TABLE supasaasy.entities
    ADD CONSTRAINT entities_app_collection_external_unique
    UNIQUE (app_key, collection_key, external_id);
  END IF;
END $$;

-- Indexes
CREATE INDEX IF NOT EXISTS idx_entities_app_key ON supasaasy.entities (app_key);
CREATE INDEX IF NOT EXISTS idx_entities_collection_key ON supasaasy.entities (collection_key);
CREATE INDEX IF NOT EXISTS idx_entities_external_id ON supasaasy.entities (external_id);
CREATE INDEX IF NOT EXISTS idx_entities_app_collection ON supasaasy.entities (app_key, collection_key);
CREATE INDEX IF NOT EXISTS idx_entities_updated_at ON supasaasy.entities (updated_at DESC);

-- Timestamp auto-update trigger
CREATE OR REPLACE FUNCTION supasaasy.update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
  NEW.updated_at = now();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

DROP TRIGGER IF EXISTS entities_update_updated_at ON supasaasy.entities;
CREATE TRIGGER entities_update_updated_at
  BEFORE UPDATE ON supasaasy.entities
  FOR EACH ROW
  EXECUTE FUNCTION supasaasy.update_updated_at_column();

-- Grant permissions
GRANT SELECT ON supasaasy.entities TO authenticated;
GRANT ALL ON supasaasy.entities TO service_role;

-- =============================================================================
-- Sync State Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.sync_state (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  app_key TEXT NOT NULL,
  collection_key TEXT NOT NULL,
  last_synced_at TIMESTAMPTZ NOT NULL,
  last_sync_metadata JSONB NOT NULL DEFAULT '{}',
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

COMMENT ON TABLE supasaasy.sync_state IS 'Tracks sync state for each app/collection combination';

-- Unique constraint
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_constraint
    WHERE conname = 'sync_state_app_collection_unique'
  ) THEN
    ALTER TABLE supasaasy.sync_state
    ADD CONSTRAINT sync_state_app_collection_unique
    UNIQUE (app_key, collection_key);
  END IF;
END $$;

-- Indexes
CREATE INDEX IF NOT EXISTS idx_sync_state_app_key ON supasaasy.sync_state (app_key);

-- Trigger for updated_at
DROP TRIGGER IF EXISTS sync_state_update_updated_at ON supasaasy.sync_state;
CREATE TRIGGER sync_state_update_updated_at
  BEFORE UPDATE ON supasaasy.sync_state
  FOR EACH ROW
  EXECUTE FUNCTION supasaasy.update_updated_at_column();

-- Grant permissions
GRANT SELECT ON supasaasy.sync_state TO authenticated;
GRANT ALL ON supasaasy.sync_state TO service_role;

-- =============================================================================
-- Webhook Logs Table
-- =============================================================================

CREATE TABLE IF NOT EXISTS supasaasy.webhook_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  app_key TEXT,
  request_method TEXT NOT NULL,
  request_path TEXT NOT NULL,
  request_headers JSONB NOT NULL DEFAULT '{}',
  request_body JSONB,
  response_status INTEGER NOT NULL,
  response_body JSONB,
  error_message TEXT,
  processing_duration_ms INTEGER,
  created_at TIMESTAMPTZ NOT NULL DEFAULT now()
);

COMMENT ON TABLE supasaasy.webhook_logs IS 'Logs webhook requests for debugging and auditing';
COMMENT ON COLUMN supasaasy.webhook_logs.app_key IS 'App key the webhook was received for (nullable for invalid requests)';
COMMENT ON COLUMN supasaasy.webhook_logs.request_method IS 'HTTP method of the webhook request';
COMMENT ON COLUMN supasaasy.webhook_logs.request_path IS 'URL path of the webhook request';
COMMENT ON COLUMN supasaasy.webhook_logs.request_headers IS 'Request headers (sensitive values redacted)';
COMMENT ON COLUMN supasaasy.webhook_logs.request_body IS 'Request body payload';
COMMENT ON COLUMN supasaasy.webhook_logs.response_status IS 'HTTP response status code';
COMMENT ON COLUMN supasaasy.webhook_logs.response_body IS 'Response body payload';
COMMENT ON COLUMN supasaasy.webhook_logs.error_message IS 'Error message if processing failed';
COMMENT ON COLUMN supasaasy.webhook_logs.processing_duration_ms IS 'Processing duration in milliseconds';
COMMENT ON COLUMN supasaasy.webhook_logs.created_at IS 'Timestamp when the log entry was created';

-- Indexes for efficient querying
CREATE INDEX IF NOT EXISTS idx_webhook_logs_app_key ON supasaasy.webhook_logs (app_key);
CREATE INDEX IF NOT EXISTS idx_webhook_logs_response_status ON supasaasy.webhook_logs (response_status);
CREATE INDEX IF NOT EXISTS idx_webhook_logs_created_at ON supasaasy.webhook_logs (created_at DESC);
CREATE INDEX IF NOT EXISTS idx_webhook_logs_app_key_created_at ON supasaasy.webhook_logs (app_key, created_at DESC);

-- Grant permissions
GRANT SELECT ON supasaasy.webhook_logs TO authenticated;
GRANT ALL ON supasaasy.webhook_logs TO service_role;
`;

// =============================================================================
// Migration Generation
// =============================================================================

/**
 * Options for getMigrations
 */
export interface GetMigrationsOptions {
  /**
   * Whether to include a header comment with version and timestamp.
   * Default: true
   */
  includeHeader?: boolean;

  /**
   * Custom version string to include in the header.
   * Default: package version
   */
  version?: string;
}

/**
 * Generate SQL migrations for SupaSaaSy based on the configuration.
 *
 * This function generates a complete SQL migration file that includes:
 * 1. The core SupaSaaSy schema (entities table, sync_state table, webhook_logs table)
 * 2. Connector-specific migrations for all connectors used in the configuration
 *
 * The generated SQL uses idempotent statements (CREATE IF NOT EXISTS, CREATE OR REPLACE)
 * so it can be safely re-run.
 *
 * @param config The SupaSaaSy configuration
 * @param options Generation options
 * @returns Complete SQL migration string
 *
 * @example
 * ```typescript
 * import { getMigrations } from 'supasaasy';
 * import config from '../supasaasy.config.ts';
 *
 * const sql = getMigrations(config);
 * await Deno.writeTextFile(
 *   'supabase/migrations/00000000000001_supasaasy.sql',
 *   sql
 * );
 * console.log('Migration file generated');
 * ```
 */
export async function getMigrations(
  config: SupaSaaSyConfig,
  options: GetMigrationsOptions = {},
): Promise<string> {
  const { includeHeader = true, version = '1.0.0' } = options;

  const parts: string[] = [];

  // Add header if requested
  if (includeHeader) {
    parts.push(`-- ============================================================================
-- SupaSaaSy Migration
-- Version: ${version}
-- Generated: ${new Date().toISOString()}
-- ============================================================================
-- This migration file was generated by @supasaasy/core getMigrations().
-- It includes the core schema and connector-specific migrations.
-- All statements are idempotent and can be safely re-run.
-- ============================================================================
`);
  }

  // Add core schema
  parts.push(CORE_SCHEMA_SQL);

  // Collect unique connectors from config
  const connectorNames = new Set(config.apps.map((app) => app.connector));

  // Add connector-specific migrations
  for (const connectorName of connectorNames) {
    const connector = await getConnector(connectorName);
    if (!connector) {
      console.warn(`Connector not found: ${connectorName}`);
      continue;
    }

    const migrationFiles = connector.metadata.migrations || [];
    if (migrationFiles.length === 0) {
      continue;
    }

    parts.push(`
-- ============================================================================
-- ${connector.metadata.displayName} Connector Migrations
-- Version: ${connector.metadata.version}
-- ============================================================================
`);

    // Load and append each migration file
    // In the library context, migrations are bundled as text
    for (const _filename of migrationFiles) {
      // The actual SQL content would need to be embedded or loaded
      // For now, we'll reference that connectors should export their migration SQL
      const migrationSql = await loadConnectorMigration(connectorName, _filename);
      if (migrationSql) {
        parts.push(migrationSql);
      }
    }
  }

  return parts.join('\n');
}

/**
 * Load a connector's migration SQL file.
 * This is a helper that reads migration files from the package.
 */
async function loadConnectorMigration(
  connectorName: string,
  filename: string,
): Promise<string | null> {
  // In the library context, we need to load migration files
  // This can be done via dynamic import or by embedding the SQL

  // Try to load from the package's migrations directory
  // The path is relative to the module's location
  const migrationPaths = [
    // Relative to this file
    `../connectors/${connectorName}/migrations/${filename}`,
    // Absolute path for development
    `./src/connectors/${connectorName}/migrations/${filename}`,
  ];

  for (const path of migrationPaths) {
    try {
      // Try to construct the full URL for the migration file
      const url = new URL(path, import.meta.url);
      const response = await fetch(url);
      if (response.ok) {
        return await response.text();
      }
    } catch {
      // Try reading as file path
      try {
        const modulePath = import.meta.url.replace('file://', '').replace('/get-migrations.ts', '');
        const filePath = `${modulePath}/../connectors/${connectorName}/migrations/${filename}`;
        return await Deno.readTextFile(filePath);
      } catch {
        // Continue to next path
      }
    }
  }

  console.warn(`Could not load migration file: ${connectorName}/${filename}`);
  return null;
}

/**
 * Get the core schema SQL without any connector-specific migrations.
 * Useful for minimal setups or testing.
 */
export function getCoreSchema(): string {
  return CORE_SCHEMA_SQL;
}
